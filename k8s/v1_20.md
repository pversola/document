# Install Kubunetes v.1.20

First provision 3 virtual machine with Ubuntu 20.04

| Hostname   | IP address     | CPU | Memory | Remark                            |
| ---------- | -------------- | --- | ------ | --------------------------------- |
| k8s-master | 192.168.200.10 |     |        | Virtual IP address for keepalived |
| master01   | 192.168.200.11 | 2   | 4      | Kubernetes control plane          |
| master02   | 192.168.200.12 | 2   | 4      | Kubernetes control plane          |
| master03   | 192.168.200.13 | 2   | 4      | Kubernetes control plane          |
| worker01   | 192.168.200.21 | 2   | 4      | Kubernetes worker node            |
| worker02   | 192.168.200.22 | 2   | 4      | Kubernetes worker node            |

### Step 1: Update Ubuntu Server (On all nodes)

```
$ sudo apt update
$ sudo apt -y upgrade && sudo systemctl reboot
```

Fix Hosts (All hosts)

```
sudo -i
echo '192.168.200.10 kube-master' >> /etc/hosts
echo '192.168.200.11 master01' >> /etc/hosts
echo '192.168.200.12 master02' >> /etc/hosts
echo '192.168.200.13 master03' >> /etc/hosts
echo '192.168.200.21 worker01' >> /etc/hosts
echo '192.168.200.22 worker02' >> /etc/hosts
```

Configure Aunthentication

```
### All nodes ###
# ssh-keygen -t rsa
# cat /root/.ssh/id_rsa.pub

### In master01 node copy SSH Key from all nodes paste in authorized_keys file ###
# vi /root/.ssh/authorized_keys

### Other node copy authorized_keys from first node ###
# scp -P 42200 root@master01:/root/.ssh/authorized_keys /root/.ssh/authorized_keys
```

### Step 2: Install kubelet, kubeadm and kubectl (On all nodes)

Set up the repository

```
$ sudo apt update
$ sudo apt -y install \
    apt-transport-https \
    ca-certificates \
    curl \
    gnupg-agent \
    software-properties-common \
    net-tools \
    nfs-common \
    telnet

$ curl -fsSL https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -

$ cat <<EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list
deb https://apt.kubernetes.io/ kubernetes-xenial main
EOF
```

Install a latest version

```
$ sudo apt update
$ sudo apt install -y kubelet kubeadm kubectl
$ sudo apt-mark hold kubelet kubeadm kubectl
```

Install a specific version

```
$ apt-cache madison kubelet
   kubelet |  1.21.0-00 | https://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
   kubelet |  1.20.6-00 | https://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
   kubelet |  1.20.5-00 | https://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
   kubelet |  1.20.4-00 | https://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
  ...

$ export KUBEVERSION=1.20.10-00
$ sudo apt update
$ sudo apt install -y kubelet=$KUBEVERSION kubeadm=$KUBEVERSION kubectl=$KUBEVERSION
$ sudo apt-mark hold kubelet kubeadm kubectl
$ unset KUBEVERSION
```

### Step 3: Disable Swap (On all nodes)

```
$ sudo sed -ri '/\sswap\s/s/^#?/#/' /etc/fstab
$ sudo swapoff -a
```

### Step 4: Install CRI-O (On all nodes)

```
$ cat <<EOF | sudo tee /etc/modules-load.d/containerd.conf
overlay
br_netfilter
EOF

$ sudo modprobe overlay
$ sudo modprobe br_netfilter

### Setup required sysctl params, these persist across reboots. ###
$ cat <<EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
EOF

$ sudo -i
# echo '1' > /proc/sys/net/bridge/bridge-nf-call-iptables
# logout
```

Reload sysctl

```
$ sudo sysctl --system
```

Add cri-o repo

```
$ export OS=xUbuntu_20.04 && export VERSION=1.20

$ cat <<EOF | sudo tee /etc/apt/sources.list.d/devel:kubic:libcontainers:stable.list
deb https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/$OS/ /
EOF

$ cat <<EOF | sudo tee /etc/apt/sources.list.d/devel:kubic:libcontainers:stable:cri-o:$VERSION.list
deb http://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable:/cri-o:/$VERSION/$OS/ /
EOF

$ curl -fsSL https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/$OS/Release.key | sudo apt-key --keyring /etc/apt/trusted.gpg.d/libcontainers.gpg add -

$ curl -fsSL https://download.opensuse.org/repositories/devel:kubic:libcontainers:stable:cri-o:$VERSION/$OS/Release.key | sudo apt-key --keyring /etc/apt/trusted.gpg.d/libcontainers-cri-o.gpg add -

$ unset OS && unset VERSION
```

Install CRI-O

```
$ sudo apt update
$ sudo apt -y install \
    cri-o \
    cri-o-runc
```

Enable Services

```
$ sudo systemctl daemon-reload
$ sudo systemctl enable crio --now
```

Cgroup management implementation used for the runtime.

```
$ cat <<EOF | sudo tee /etc/crio/crio.conf.d/01-crio-runc.conf
[crio.runtime.runtimes.runc]
runtime_path = "/usr/lib/cri-o-runc/sbin/runc"
runtime_type = "oci"
runtime_root = "/run/runc"
EOF

$ cat <<EOF | sudo tee /etc/crio/crio.conf.d/01-log-level.conf
[crio.runtime]
log_level = "info"
EOF

$ cat <<EOF | sudo tee /etc/crio/crio.conf.d/02-cgroup-manager.conf
[crio.runtime]
conmon_cgroup = "pod"
cgroup_manager = "cgroupfs"
EOF
```

Add a default kubelet

```
$ cat <<EOF | sudo tee /etc/default/kubelet
KUBELET_EXTRA_ARGS=--feature-gates="AllAlpha=false,RunAsGroup=true" --container-runtime=remote --cgroup-driver=systemd --container-runtime-endpoint='unix:///var/run/crio/crio.sock' --runtime-request-timeout=5m
EOF
```

```
$ cat <<EOF | sudo tee /etc/default/kubelet
KUBELET_EXTRA_ARGS=--container-runtime=remote --container-runtime-endpoint='unix:///var/run/crio/crio.sock' --tls-cipher-suites="TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384"
EOF

$ cat <<EOF | sudo tee /etc/default/etcd
ETCD_EXTRA_ARGS=--cipher-suites="TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384"
EOF

$ cat <<EOF | sudo tee /etc/default/kube-apiserver
KUBE_APISERVER_EXTRA_ARGS=--tls-cipher-suites="TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384"
EOF
```

Restart crio service

```
$ sudo systemctl restart crio kubelet
```

Pull necessary container images

```
$ sudo kubeadm config images pull
```

### Step 5: High-Availability (On Master/Control plane node)

**_If user control plane single node. Please skip step_**

**5.1: Install Keepalived for master node**

```
$ sudo apt -y install keepalived
```

Config Keepalived on master01

```
$ cat <<EOF | sudo tee /etc/keepalived/keepalived.conf
vrrp_instance VI_1 {
    interface eth0
    state MASTER
    priority 200

    virtual_router_id 33
    advert_int 1

    unicast_src_ip 192.168.200.11
    unicast_peer {
        192.168.200.12
        192.168.200.13
    }

    virtual_ipaddress {
        192.168.200.10
    }

    authentication {
        auth_type PASS
        auth_pass EhazK1Y2MBK37gZktTl1zrUUuBk
    }
}
EOF
```

Config Keepalived on master02

```
$ cat <<EOF | sudo tee /etc/keepalived/keepalived.conf
vrrp_instance VI_1 {
    interface eth0
    state BACKUP
    priority 199

    virtual_router_id 33
    advert_int 1

    unicast_src_ip 192.168.200.12
    unicast_peer {
        192.168.200.11
        192.168.200.13
    }

    virtual_ipaddress {
        192.168.200.10
    }

    authentication {
        auth_type PASS
        auth_pass EhazK1Y2MBK37gZktTl1zrUUuBk
    }
}
EOF
```

Config Keepalived on master03

```
$ cat <<EOF | sudo tee /etc/keepalived/keepalived.conf
vrrp_instance VI_1 {
    interface eth0
    state BACKUP
    priority 198

    virtual_router_id 33
    advert_int 1

    unicast_src_ip 192.168.200.13
    unicast_peer {
        192.168.200.11
        192.168.200.12
    }

    virtual_ipaddress {
        192.168.200.10
    }

    authentication {
        auth_type PASS
        auth_pass EhazK1Y2MBK37gZktTl1zrUUuBk
    }
}
EOF
```

Enable and start the keepalived service

```
$ sudo systemctl enable --now keepalived
```

Check virtual ip

```
$ ip addr show eth0
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000
    link/ether 00:15:5d:a6:91:10 brd ff:ff:ff:ff:ff:ff
    inet 192.168.200.11/24 brd 192.168.200.255 scope global eth0
       valid_lft forever preferred_lft forever
    inet 192.168.200.10/32 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::215:5dff:fea6:9110/64 scope link
```

### Step 6: Initialize master node (On Master/Control plane node)

Initialize on master01

```
$ sudo kubeadm init \
  --pod-network-cidr "10.244.0.0/16" \
  --service-dns-domain "kube-master" \
  --control-plane-endpoint "kube-master:6443" \
  --upload-certs \
  --cri-socket=unix:///var/run/crio/crio.sock
```

**_For master single node_**

```
$ sudo kubeadm init --pod-network-cidr=10.244.0.0/16 --cri-socket=unix:///var/run/crio/crio.sock
```

Result success.

```
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of the control-plane node running the following command on each as root:

  kubeadm join k8s-master:6443 --token 4rzyb9.d97yq3j3982wo94z \
    --discovery-token-ca-cert-hash sha256:60c1a1ec9092ed6807899262252bda397c5a233f0bbe6c94f365a01d6689b04a \
    --control-plane --certificate-key 319aad55b5a998f49dd71ead21b9957b3fb352d342ffdf61a2d18ee5695c96ac

Please note that the certificate-key gives access to cluster sensitive data, keep it secret!
As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use
"kubeadm init phase upload-certs --upload-certs" to reload certs afterward.

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join k8s-master:6443 --token 4rzyb9.d97yq3j3982wo94z \
    --discovery-token-ca-cert-hash sha256:60c1a1ec9092ed6807899262252bda397c5a233f0bbe6c94f365a01d6689b04a
```

To make kubectl work for your non-root user

```
$ mkdir -p $HOME/.kube
$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
$ sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

Test kubectl

```
kubectl get nodes
NAME       STATUS   ROLES                  AGE   VERSION
master01   Ready    control-plane,master   88s   v1.20.6
```

### Step 7: Install network plugin (CNI) configuration (On Master/Control plane node)

```
$ kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
```

Check cluster status: `$ kubectl cluster-info`  
Check all pods are running in kube-system: `watch kubectl get pods -n kube-system`  
Confirm master node is ready: `$ kubectl get nodes`

**_FIX:_** remove old calico configs from kubernetes without `kubeadm reset`

```
$ sudo ip route flush proto bird
$ sudo ip link list | grep cali | awk '{print $2}' | cut -c 1-15 | xargs -I {} ip link delete {}
$ sudo modprobe -r ipip
$ sudo rm /etc/cni/net.d/10-calico.conflist && rm /etc/cni/net.d/calico-kubeconfig
$ sudo service kubelet restart
```

### Step 8: Add master node

**_If user control plane single node. Please skip to step 9_**  
**_If ypu note commad from step 6 and token is not expire. Please skip to step 8.5_**  
Run the following command on control-plane nodes **_master02_** and **_master03_**.

**Step 8.1: Get Token (On Master/Control plane node)**

```
$ sudo kubeadm token list
$ sudo kubeadm token create --print-join-command
```

**Step 8.2: Get Discovery Token CA cert Hash (On Master/Control plane node)**

```
$ openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //'
```

**Step 8.3: Get API Server Advertise address (On Master/Control plane node)**

```
$ kubectl cluster-info
Kubernetes control plane is running at https://<HOST>:<PORT>
KubeDNS is running at https://<HOST>:<PORT>/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
```

**Step 8.4: Get Certificate Key (On Master/Control plane node)**

```
$ kubeadm init phase upload-certs --upload-certs
```

**Step 8.5: Join a new Kubernetes Master Node a Cluster**

```
$ sudo kubeadm join <ENDPOINT IP>:<PORT> --token <TOKEN> \
    --discovery-token-ca-cert-hash sha256:<DISCOVERY TOKEN> \
    --control-plane --certificate-key <CERTIFICATE KEY>
```

To make kubectl work for your non-root user

```
$ mkdir -p $HOME/.kube
$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
$ sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

**Step 8.6: Disabled scheduling from the cluster (On Master/Control plane node)**

```
$ kubectl taint nodes --all node-role.kubernetes.io/master-
```

### Step 9: Add worker node

**_If ypu note commad from step 6 and token is not expire. Please skip to step 9.4_**  
Run the following command on nodes **_worker01_** and **_worker02_**.

**Step 9.1: Get Token (On Master/Control plane node)**

```
$ sudo kubeadm token list
$ sudo kubeadm token create --print-join-command
```

**Step 9.2: Get Discovery Token CA cert Hash (On Master/Control plane node)**

```
$ openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //'
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
```

**Step 9.3: Get API Server Advertise address (On Master/Control plane node)**

```
$ kubectl cluster-info
Kubernetes control plane is running at https://k8s-master:6443
KubeDNS is running at https://k8s-master:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
```

**Step 9.4: Join a new Kubernetes Worker Node a Cluster**

```
$ sudo kubeadm join <ENDPOINT IP>:<PORT> --token <TOKEN> \
    --discovery-token-ca-cert-hash sha256:<DISCOVERY TOKEN>
```

### Clean up

**Remove the node**

```
kubectl drain <node name> --delete-local-data --force --ignore-daemonsets
```

Reset the state installed by `kubeadm`

```
kubeadm reset
```

The reset process does not reset or clean up iptables rules

```
sudo iptables -F && sudo iptables -t nat -F && sudo iptables -t mangle -F && sudo iptables -X
```

Reset the IPVS tables

```
sudo ipvsadm -C
```

Remove the node.

```
kubectl delete node <node name>
```

## How to upgrade to 1.20.x

### Upgrading control plane nodes

**Step 1: upgrade kubeadm (On Master/Control plane node)**

```
$ apt-cache madison kubeadm
kubeadm |  1.20.6-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
kubeadm |  1.20.5-00 | http://apt.kubernetes.io kubernetes-xenial/main amd64 Packages
...

$ export FileVersion=1.20.6-00
$ export KubeVersion=v1.20.6
$ sudo apt update
$ sudo apt update && sudo apt install -y --allow-change-held-packages kubeadm=$FileVersion
```

**Step 2: Verify the kubeadm version (On Master/Control plane node)**

```
$ sudo kubeadm version
```

**Step 3: Verify the upgrade plan (On Master/Control plane node)**

```
$ kubeadm upgrade plan
```

**Step 4: Choose a version to upgrade to (On Master/Control plane node)**

```
$ kubeadm upgrade apply $KubeVersion
```

**Step 5: Drain the node (On Master/Control plane node)**

```
$ kubectl drain master01 --ignore-daemonsets
```

**Step 6: Upgrade kubelet and kubectl (On Master/Control plane node)**

```
$ sudo apt update && sudo apt install -y --allow-change-held-packages kubelet=$FileVersion kubectl=$FileVersion
```

**Step 7: Restart the kubelet (On Master/Control plane node)**

```
$ sudo systemctl daemon-reload
$ sudo systemctl restart kubelet
```

**Step 8: Uncordon the node (On Master/Control plane node)**

```
$ kubectl uncordon master01
```

**Step 9: Verify the version (On Master/Control plane node)**

```
$ kubectl get nodes
NAME       STATUS                     ROLES                  AGE   VERSION
master01   Ready                      control-plane,master   31m   v1.20.6
master02   Ready,SchedulingDisabled   control-plane,master   28m   v1.20.4
master03   Ready,SchedulingDisabled   control-plane,master   28m   v1.20.4
```

### Upgrade worker nodes

**Step 1: Upgrade kubeadm: (On worker node)**

```
$ export FileVersion=1.20.6-00
$ export KubeVersion=v1.20.6
$ sudo apt update
$ sudo apt update && sudo apt install -y --allow-change-held-packages kubeadm=$FileVersion
```

**Step 2: Upgrades the local kubelet configuration (On worker node)**

```
$ kubeadm upgrade node
```

**Step 3: Drain the node (On Master/Control plane node)**

```
$ kubectl drain worker01  --ignore-daemonsets
```

**Step 4: Upgrade kubelet and kubectl (On worker node)**

```
$ sudo apt update && sudo apt install -y --allow-change-held-packages kubelet=$FileVersion kubectl=$FileVersion
```

**Step 5: Restart the kubelet (On worker node)**

```
$ sudo systemctl daemon-reload
$ sudo systemctl restart kubelet
```

**Step 6: Uncordon the node (On Master/Control plane node)**

```
$ kubectl uncordon worker01
```

**Step 7: Verify the status of the cluster (On Master/Control plane node)**

```
$ kubectl get nodes
```

## How to install HPA

```
$ kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
```

Depending on your cluster setup, you may also need to change flags passed to the Metrics Server container. Most useful flags:

`--kubelet-preferred-address-types` - The priority of node address types used when determining an address for connecting to a particular node (default [Hostname, InternalDNS, InternalIP, ExternalDNS, ExternalIP])  
`--kubelet-insecure-tls` - Do not verify the CA of serving certificates presented by Kubelets. For testing purposes only.  
`--requestheader-client-ca-file` - Specify a root certificate bundle for verifying client certificates on incoming requests.

```
$ kubectl edit deploy -n kube-system metrics-server
[...]
spec:
  [...]
  template:
    [...]
    spec:
      containers:
      - args:
        - --cert-dir=/tmp
        - --secure-port=4443
        - --kubelet-insecure-tls
        - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
        - --kubelet-use-node-status-port
        image: k8s.gcr.io/metrics-server/metrics-server:v0.4.2

[...]
```

## How to install Dashboard

```
$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.1.0/aio/deploy/recommended.yaml

$ cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin-user
  namespace: kubernetes-dashboard
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: admin-user
  namespace: kubernetes-dashboard
EOF

$ echo "" ; kubectl get secret -n kubernetes-dashboard $(kubectl get serviceaccount admin-user -n kubernetes-dashboard -o jsonpath="{.secrets[0].name}") -o jsonpath="{.data.token}" | base64 --decode ; echo ""


$ kubectl -n kubernetes-dashboard edit svc kubernetes-dashboard
[...]
spec:
  [...]
  ports:
  - port: 443
    nodePort: 32000
    protocol: TCP
    targetPort: 8443
  [...]
  type: NodePort
[...]
```

## How to install Monitor

```
$ git clone https://github.com/prometheus-operator/kube-prometheus.git
$ cd kube-prometheus/
$ kubectl create -f manifests/setup -f manifests

$ kubectl -n monotoring edit svc prometheus-k8s
[...]
spec:
[...]
ports:

- name: web
  nodePort: 32102
  port: 9090
  protocol: TCP
  targetPort: web

[...]
type: NodePort
[...]

$ kubectl -n monotoring edit svc alertmanager-main
[...]
spec:
[...]
ports:

- name: web
  nodePort: 32103
  port: 9093
  protocol: TCP
  targetPort: web
  [...]
  type: NodePort
  [...]

$ kubectl -n monotoring edit svc grafana
[...]
spec:
[...]
ports:

- name: http
  nodePort: 32101
  port: 3000
  protocol: TCP
  targetPort: http
  [...]
  type: NodePort
  [...]

```